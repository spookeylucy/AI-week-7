{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8585d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPAS Dataset Bias Audit with AIF360\n",
    "\n",
    "# Install AIF360\n",
    "!pip install aif360\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.postprocessing import RejectOptionClassification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load COMPAS dataset\n",
    "dataset_orig = CompasDataset(protected_attribute_names=['race'],\n",
    "                             privileged_classes=[['Caucasian']],\n",
    "                             features_to_drop=['sex', 'name', 'id'])\n",
    "\n",
    "# Split into train and test\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)\n",
    "\n",
    "# Baseline Fairness Metrics\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "    unprivileged_groups=[{'race': 1}], \n",
    "    privileged_groups=[{'race': 0}])\n",
    "\n",
    "print(\"Mean Difference (privileged - unprivileged):\", metric_orig_train.mean_difference())\n",
    "print(\"Disparate Impact:\", metric_orig_train.disparate_impact())\n",
    "\n",
    "# Reweighing (Mitigation)\n",
    "RW = Reweighing(unprivileged_groups=[{'race': 1}], privileged_groups=[{'race': 0}])\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train)\n",
    "\n",
    "# Train Logistic Model\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(dataset_transf_train.features)\n",
    "y_train = dataset_transf_train.labels.ravel()\n",
    "w_train = dataset_transf_train.instance_weights\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train, sample_weight=w_train)\n",
    "\n",
    "# Test on Original Test Set\n",
    "X_test = scaler.transform(dataset_orig_test.features)\n",
    "y_pred = model.predict(X_test)\n",
    "dataset_pred = dataset_orig_test.copy()\n",
    "dataset_pred.labels = y_pred\n",
    "\n",
    "# Evaluate Bias & Accuracy\n",
    "classified_metric = ClassificationMetric(dataset_orig_test, dataset_pred,\n",
    "                                         unprivileged_groups=[{'race': 1}],\n",
    "                                         privileged_groups=[{'race': 0}])\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(dataset_orig_test.labels, y_pred))\n",
    "print(\"Disparate Impact:\", classified_metric.disparate_impact())\n",
    "print(\"Equal Opportunity Diff:\", classified_metric.equal_opportunity_difference())\n",
    "print(\"Average Odds Diff:\", classified_metric.average_odds_difference())\n",
    "print(\"False Positive Rate Diff:\", classified_metric.false_positive_rate_difference())\n",
    "\n",
    "# Visualization\n",
    "labels = ['Disparate Impact', 'Equal Opportunity Diff', 'Avg Odds Diff', 'FPR Diff']\n",
    "values = [classified_metric.disparate_impact(),\n",
    "          classified_metric.equal_opportunity_difference(),\n",
    "          classified_metric.average_odds_difference(),\n",
    "          classified_metric.false_positive_rate_difference()]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(labels, values, color='salmon')\n",
    "plt.title('Fairness Metrics After Mitigation')\n",
    "plt.axhline(0, color='gray', linestyle='--')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
