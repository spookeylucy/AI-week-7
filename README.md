# Part 2 Case 1(Charity Muigai)

# AI Hiring Bias Case Study: Amazon's Recruiting Tool

## Overview
This case study examines Amazon's AI recruiting tool that penalized female candidates, analyzing the bias sources and proposing comprehensive solutions for fair AI hiring systems.

## The Problem
Amazon's AI recruiting tool exhibited significant gender bias, systematically downgrading female candidates due to training on historically biased data from a male-dominated industry.

## Root Cause Analysis
**Primary Bias Source**: Biased training data
- 10+ years of historical resumes from male-dominated tech industry
- AI learned to replicate discriminatory hiring patterns
- Gender-coded language and experiences were penalized

## Solutions Framework

### ðŸ”§ Technical Fixes
1. **Data Preprocessing**: Remove gendered language, augment with synthetic balanced data
2. **Algorithmic Debiasing**: Implement adversarial training and fairness constraints
3. **Human Oversight**: Structured blind review with diverse evaluation panels

### ðŸ“Š Fairness Metrics
- **Statistical Parity**: Equal selection rates across gender groups
- **Equalized Odds**: Consistent true/false positive rates
- **Outcome Tracking**: Monitor hiring, performance, and retention rates
- **Process Auditing**: Regular feature importance and intersectional analysis

## Key Takeaways
- Historical bias in training data perpetuates discrimination
- Multiple intervention points required (data, algorithms, process)
- Continuous monitoring essential for maintaining fairness
- Human oversight remains critical for ethical AI deployment

## Next Steps
1. Implement proposed technical solutions
2. Establish regular bias auditing schedule
3. Train hiring teams on fair AI practices
4. Monitor outcomes across all demographic groups

---
*This analysis provides a framework for identifying and addressing bias in AI hiring systems.*
